{
 "nbformat_minor": 1,
 "cells": [
  {
   "source": [
    "# ."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": "# 1. Linear Regression\nLinear Regression algorithm will use the data points to find the best fit line to model the data. A line can be represented by the equation, y = m*x + c where y is the dependent variable and x is the independent variable. Basic calculus theories are applied to find the values for m and c using the given data set.\nLinear Regression has 2 types as Simple Linear Regression where only 1 independent variable is used and Multiple Linear Regression where multiple independent variables are defined.\n\nâscikit-learnâ is a simple and efficient tool using for machine learning in python. Following is an implementation of Linear Regression using scikit-learn.",
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "execution_count": 1,
   "cell_type": "code",
   "source": "from sklearn import linear_model, datasets\n\n#digit dataset from sklearn\ndigits = datasets.load_digits()\n\n#create the LinearRegression model\nclf = linear_model.LinearRegression()\n\n#set training set\nx, y = digits.data[:-1], digits.target[:-1]\n\n#train model\nclf.fit(x, y)\n\n#predict\ny_pred = clf.predict([digits.data[-1]])\ny_true = digits.target[-1]\n\nprint(y_pred)\nprint(y_true)",
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[8.86342983]\n8\n"
     ],
     "output_type": "stream"
    }
   ],
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   }
  },
  {
   "execution_count": 2,
   "cell_type": "code",
   "source": "print(len(digits[\"data\"]))\nprint(digits.data.shape)\nprint(digits.keys())\n\ni=100\nprint(clf)\nprint(digits[\"data\"][i])\nprint(digits[\"target\"][i])\n\nimport matplotlib.pyplot as plt \nplt.gray() \nplt.matshow(digits.images[i]) \nplt.show() ",
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "1797\n(1797, 64)\ndict_keys(['data', 'target', 'target_names', 'images', 'DESCR'])\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n         normalize=False)\n[ 0.  0.  0.  2. 13.  0.  0.  0.  0.  0.  0.  8. 15.  0.  0.  0.  0.  0.\n  5. 16.  5.  2.  0.  0.  0.  0. 15. 12.  1. 16.  4.  0.  0.  4. 16.  2.\n  9. 16.  8.  0.  0.  0. 10. 14. 16. 16.  4.  0.  0.  0.  0.  0. 13.  8.\n  0.  0.  0.  0.  0.  0. 13.  6.  0.  0.]\n4\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 480x480 with 1 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   }
  },
  {
   "execution_count": 2,
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   }
  },
  {
   "execution_count": 3,
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import datasets, linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Load the diabetes dataset\ndiabetes = datasets.load_diabetes()\n\n\n# Use only one feature\ndiabetes_X = diabetes.data[:, np.newaxis, 2]\n\n# Split the data into training/testing sets\ndiabetes_X_train = diabetes_X[:-20]\ndiabetes_X_test = diabetes_X[-20:]\n\n# Split the targets into training/testing sets\ndiabetes_y_train = diabetes.target[:-20]\ndiabetes_y_test = diabetes.target[-20:]\n\n# Create linear regression object\nregr = linear_model.LinearRegression()\n\n# Train the model using the training sets\nregr.fit(diabetes_X_train, diabetes_y_train)\n\n# Make predictions using the testing set\ndiabetes_y_pred = regr.predict(diabetes_X_test)\n\n# The coefficients\nprint('Coefficients: \\n', regr.coef_)\n# The mean squared error\nprint(\"Mean squared error: %.2f\"\n      % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n# Explained variance score: 1 is perfect prediction\nprint('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))\n\n# Plot outputs\nplt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\nplt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n\n#plt.xticks(())\n#plt.yticks(())\n\nplt.show()",
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Coefficients: \n [938.23786125]\nMean squared error: 2548.07\nVariance score: 0.47\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   }
  },
  {
   "execution_count": 4,
   "cell_type": "code",
   "source": "print(len(diabetes[\"data\"]))\nprint(diabetes.data.shape)\nprint(diabetes.keys())\n\ni=100\nprint(regr)\nprint(diabetes[\"data\"][i])\nprint(diabetes[\"target\"][i])\n\n",
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "442\n(442, 10)\ndict_keys(['data', 'target', 'DESCR', 'feature_names', 'data_filename', 'target_filename'])\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n         normalize=False)\n[ 0.01628068 -0.04464164  0.01750591 -0.02288496  0.06034892  0.0444058\n  0.03023191 -0.00259226  0.03723201 -0.0010777 ]\n128.0\n"
     ],
     "output_type": "stream"
    }
   ],
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   }
  },
  {
   "source": "## 2. SVM (Support Vector Machine)\nThis belongs to classification type algorithm. The algorithm will separate the data points using a line. This line is chosen such that it will be furthermost from the nearest data points in 2 categories.\n\nIn above diagram red line is the best line since it has the most distance from the nearest points. Based on this line data points are classified into 2 groups.",
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "execution_count": 5,
   "cell_type": "code",
   "source": "from sklearn import svm, datasets\n\n#digit dataset from sklearn\ndigits = datasets.load_digits()\n\n#create the  Support Vector Classifier\nclf = svm.SVC(gamma = 0.001, C = 100)\n\n#set training set\nx, y = digits.data[:-1], digits.target[:-1]\n\n#train model\nclf.fit(x, y)\n\n#predict\ny_pred = clf.predict([digits.data[-1]])\ny_true = digits.target[-1]\n\nprint(y_pred)\nprint(y_true)\n",
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[8]\n8\n"
     ],
     "output_type": "stream"
    }
   ],
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   }
  },
  {
   "source": "## 3. KNN (K-Nearest Neighbors)\nThis is a simple algorithm which predicts unknown data point with its k nearest neighbors. The value of k is a critical factor here regarding the accuracy of prediction. It determines the nearest by calculating the distance using basic distance functions like Euclidean.\n<p>\nHowever, this algorithm needs high computation power and we need to normalize data initially to bring every data point to same range",
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "execution_count": 6,
   "cell_type": "code",
   "source": "from sklearn import datasets\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#digit dataset from sklearn\ndigits = datasets.load_digits()\n\n#create the  KNeighborsClassifier\nclf = KNeighborsClassifier(n_neighbors=6)\n\n#set training set\nx, y = digits.data[:-1], digits.target[:-1]\n\n#train model\nclf.fit(x, y)\n\n#predict\ny_pred = clf.predict([digits.data[-1]])\ny_true = digits.target[-1]\n\nprint(y_pred)\nprint(y_true)",
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[8]\n8\n"
     ],
     "output_type": "stream"
    }
   ],
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   }
  },
  {
   "source": "## 4. Logistic Regression\nLogistic Regression is used where a discreet output is expected such as the occurrence of some event (Ex. predict whether rain will occur or not). Usually, Logistic regression uses some function to squeeze values to a particular range.\n\nâSigmoidâ (Logistic function) is one of such function which has âSâ shape curve used for binary classification. It converts values to the range of 0, 1 which interpreted as a probability of occurring some event.\ny = e^(b0 + b1*x) / (1 + e^(b0 + b1*x))\n\nAbove is a simple logistic regression equation where b0, b1 are constants. While training values for these will be calculated such that the error between prediction and actual value become minimum.",
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "execution_count": 6,
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   }
  },
  {
   "source": "## 5. Decision Tree\nThis algorithm categorizes the population for several sets based on some chosen properties (independent variables) of a population. Usually, this algorithm is used to solve classification problems. Categorization is done by using some techniques such as Gini, Chi-square, entropy etc.\nLetâs consider a population of people and use decision tree algorithm to identify who like to have a credit card. For example, consider the age and marital status the properties of the population. If age>30 or a person is married, people tend to prefer credit cards much and less otherwise.\n\nThis decision tree can be further extended by identifying suitable properties to define more categories. In this example, if a person is married and he is over 30, they are more likely to have credit cards (100% preference). Testing data is used to generate this decision tree.\n",
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "execution_count": 6,
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   }
  },
  {
   "source": "## 6. K-Means\nThis is an unsupervised algorithm which provides a solution for clustering problem. The algorithm follows a procedure to form clusters which contain homogeneous data points.\n\nThe value of k is an input for the algorithm. Based on that, algorithm selects k number of centroids. Then the neighboring data points to a centroid combines with its centroid and creates a cluster. Later a new centroid is created within each cluster. Then data points near to new centroid will combine again to expand the cluster. This process is continued until centroids do not change.\n",
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "execution_count": 6,
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   }
  },
  {
   "source": "## 7. Random Forest\nRandom forest can be identified as a collection of decision trees as its name says. Each tree tries to estimate a classification and this is called as a âvoteâ. Ideally, we consider each vote from every tree and chose the most voted classification.",
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "execution_count": 6,
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   }
  },
  {
   "source": "## 8. Naive Bayes\nThis algorithm is based on the âBayesâ Theoremâ in probability. Due to that Naive Bayes can be applied only if the features are independent of each other since it is a requirement in Bayesâ Theorem. If we try to predict a flower type by its petal length and width, we can use Naive Bayes approach since both those features are independent.\n\nBayes Equation\nNaive Bayes algorithm also falls into classification type. This algorithm is mostly used when many classes exist in the problem.",
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "execution_count": 6,
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   }
  },
  {
   "source": "## 9. Dimensional Reduction Algorithms\nSome datasets may contain many variables that may cause very hard to handle. Especially nowadays data collecting in systems occur at very detailed level due to the existence of more than enough resources. In such cases, the data sets may contain thousands of variables and most of them can be unnecessary as well.\n\nIn this case, it is almost impossible to identify the variables which have the most impact on our prediction. Dimensional Reduction Algorithms are used in this kind of situations. It utilizes other algorithms like Random Forest, Decision Tree to identify the most important variables.",
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": "## 10. Gradient Boosting Algorithms\nGradient Boosting Algorithm uses multiple weak algorithms to create a more powerful accurate algorithm. Instead of using a single estimator, having multiple will create a more stable and robust algorithm.\n\nThere are several Gradient Boosting Algorithms.\n- XGBoost â uses liner and tree algorithms\n- LightGBM â uses only tree-based algorithms\n\nThe specialty of Gradient Boosting Algorithms is their higher accuracy. Further, algorithms like LightGBM has incredible high performance as well.",
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "execution_count": 6,
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   }
  },
  {
   "execution_count": 6,
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   }
  },
  {
   "execution_count": 6,
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   }
  },
  {
   "execution_count": 6,
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   }
  },
  {
   "execution_count": 7,
   "cell_type": "code",
   "source": "import pandas as pd\nimport os, bs4, requests\nimport re\n",
   "outputs": [],
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   }
  },
  {
   "execution_count": 8,
   "cell_type": "code",
   "source": "from azureml import Workspace\nws = Workspace(\n    workspace_id='809e2c97fae44071992bed8bf9ded6c5',\n    authorization_token='FwSzPaQsDr59Fg+xzJTGlrH4wUjTwWfnc++Psw2/T47KOCrZWj8FT+5JfCVGKZVw8o7RyxN1WflfNx1UnZ4vuQ==',\n    endpoint='https://studioapi.azureml.net'\n)\nds = ws.datasets['Bike Buyers']\nframe = ds.to_dataframe()\n",
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-af25cff6c8ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mazureml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWorkspace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m ws = Workspace(\n\u001b[1;32m      4\u001b[0m     \u001b[0mworkspace_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'809e2c97fae44071992bed8bf9ded6c5'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mauthorization_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'FwSzPaQsDr59Fg+xzJTGlrH4wUjTwWfnc++Psw2/T47KOCrZWj8FT+5JfCVGKZVw8o7RyxN1WflfNx1UnZ4vuQ=='\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'azureml'"
     ],
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'azureml'",
     "output_type": "error"
    }
   ],
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   }
  },
  {
   "execution_count": null,
   "cell_type": "code",
   "source": "frame.head()",
   "outputs": [],
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   }
  },
  {
   "execution_count": null,
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   }
  },
  {
   "execution_count": null,
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   }
  },
  {
   "execution_count": null,
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   }
  },
  {
   "source": "# KNN example\nhttps://stackabuse.com/k-nearest-neighbors-algorithm-in-python-and-scikit-learn/",
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "execution_count": null,
   "cell_type": "code",
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd",
   "outputs": [],
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   }
  },
  {
   "source": "## Getting data",
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "execution_count": null,
   "cell_type": "code",
   "source": "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n\n# Assign colum names to the dataset\nnames = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'Class']\n\n# Read dataset to pandas dataframe\ndataset = pd.read_csv(url, names=names)\ndataset.head()",
   "outputs": [],
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   }
  },
  {
   "source": "## Spliting X and Y into array ",
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "execution_count": null,
   "cell_type": "code",
   "source": "X = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, 4].values",
   "outputs": [],
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   }
  },
  {
   "source": "## Train Test Split",
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "execution_count": null,
   "cell_type": "code",
   "source": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)",
   "outputs": [],
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   }
  },
  {
   "source": "## Feature Scaling",
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "execution_count": null,
   "cell_type": "code",
   "source": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train)\n\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)",
   "outputs": [],
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   }
  },
  {
   "source": "## Training and prediction",
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "execution_count": null,
   "cell_type": "code",
   "source": "from sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors=5)\nclassifier.fit(X_train, y_train)",
   "outputs": [],
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   }
  },
  {
   "execution_count": null,
   "cell_type": "code",
   "source": "y_pred = classifier.predict(X_test)",
   "outputs": [],
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   }
  },
  {
   "execution_count": null,
   "cell_type": "code",
   "source": "y_pred",
   "outputs": [],
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   }
  },
  {
   "execution_count": null,
   "cell_type": "code",
   "source": "y_test",
   "outputs": [],
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   }
  },
  {
   "source": "## Evaluation",
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "execution_count": null,
   "cell_type": "code",
   "source": "from sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))",
   "outputs": [],
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   }
  },
  {
   "source": "## Comparing Error Rate with the K Value",
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "execution_count": null,
   "cell_type": "code",
   "source": "error = []\n\n# Calculating error for K values between 1 and 40\nfor i in range(1, 40):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    pred_i = knn.predict(X_test)\n    error.append(np.mean(pred_i != y_test))",
   "outputs": [],
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   }
  },
  {
   "execution_count": null,
   "cell_type": "code",
   "source": "plt.figure(figsize=(12, 6))\nplt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o',\n         markerfacecolor='blue', markersize=10)\nplt.title('Error Rate K Value')\nplt.xlabel('K Value')\nplt.ylabel('Mean Error')\nplt.show()",
   "outputs": [],
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   }
  },
  {
   "execution_count": null,
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   }
  },
  {
   "execution_count": null,
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   }
  },
  {
   "execution_count": null,
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   }
  },
  {
   "execution_count": null,
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   }
  }
 ],
 "nbformat": 4,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "mimetype": "text/x-python",
   "nbconvert_exporter": "python",
   "version": "3.4.5",
   "name": "python",
   "file_extension": ".py",
   "pygments_lexer": "ipython3",
   "codemirror_mode": {
    "version": 3,
    "name": "ipython"
   }
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [
     "# md\n",
     "\n",
     "# Algorithms\n",
     "\n",
     "###  Linear Regression\n",
     "### SVM (Support Vector Machine)\n",
     "### KNN (K-Nearest Neighbors)\n",
     "### Logistic Regression\n",
     "### Decision Tree\n",
     "### K-Means\n",
     "### Random Forest\n",
     "### Naive Bayes\n",
     "### Dimensional Reduction Algorithms\n",
     "### Gradient Boosting Algorithms\n"
    ],
    "metadata": {
     "collapsed": false
    }
   }
  }
 }
}